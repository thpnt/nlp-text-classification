{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "project_root = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117114, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from pickle\n",
    "path = os.path.join(project_root, \"datasets/processed/data.pkl\")\n",
    "data = pd.read_pickle(path)\n",
    "data['label'] = data['label'].apply(lambda x: 1 if x == 2 else x)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.gcp import load_data_from_gcs\n",
    "#from google.auth import credentials\n",
    "#from google.cloud import storage\n",
    "#service_account = os.path.join(project_root, os.getenv(\"GCP_SERVICE_ACCOUNT\"))\n",
    "#client = storage.Client.from_service_account_json(service_account)\n",
    "#\n",
    "#\n",
    "## Load data from GCS\n",
    "#bucket_name = os.getenv(\"GCP_BUCKET_NAME\")\n",
    "#file_name = os.getenv(\"GCP_DATA_PATH\")\n",
    "#data = load_data_from_gcs(bucket_name, file_name, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>corrected_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic_comment</td>\n",
       "      <td>| This was me and I haven't edited wikipedia i...</td>\n",
       "      <td>0</td>\n",
       "      <td>59acfdecb57c450ea3c2c1cd8f00af90</td>\n",
       "      <td>this was me and i have not edited wikipedia in...</td>\n",
       "      <td>[this, be, me, and, i, have, not, edit, wikipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>@chvrlesGoldie like Hov said \"we all ghetto b\"...</td>\n",
       "      <td>1</td>\n",
       "      <td>7f683aaf0fab427c84fcb4d224e76667</td>\n",
       "      <td>person like how said we all hetty b</td>\n",
       "      <td>[person, like, how, say, we, all, hetty, b]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toxic_comment</td>\n",
       "      <td>POSTSCRIPT: And this article is still really p...</td>\n",
       "      <td>1</td>\n",
       "      <td>d6c357344a0145218dc68eecd3ea69bc</td>\n",
       "      <td>postscript and this article is still really pa...</td>\n",
       "      <td>[postscript, and, this, article, be, still, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toxic_comment</td>\n",
       "      <td>The current infobox of this section, wich repr...</td>\n",
       "      <td>0</td>\n",
       "      <td>db23ee8f56ac475f945bb645861074f4</td>\n",
       "      <td>the current infobox of this section with repre...</td>\n",
       "      <td>[the, current, &lt;UNK&gt;, of, this, section, with,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>I wasn't born lastnight , I know theses hoes a...</td>\n",
       "      <td>1</td>\n",
       "      <td>95e66b8a95e54ccaa2c259d3abdcc864</td>\n",
       "      <td>i was not born lastnight i know these hoes is ...</td>\n",
       "      <td>[i, be, not, bear, &lt;UNK&gt;, i, know, these, hoe,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                               text  label  \\\n",
       "0  toxic_comment  | This was me and I haven't edited wikipedia i...      0   \n",
       "1    hate_speech  @chvrlesGoldie like Hov said \"we all ghetto b\"...      1   \n",
       "2  toxic_comment  POSTSCRIPT: And this article is still really p...      1   \n",
       "3  toxic_comment  The current infobox of this section, wich repr...      0   \n",
       "4    hate_speech  I wasn't born lastnight , I know theses hoes a...      1   \n",
       "\n",
       "                                 id  \\\n",
       "0  59acfdecb57c450ea3c2c1cd8f00af90   \n",
       "1  7f683aaf0fab427c84fcb4d224e76667   \n",
       "2  d6c357344a0145218dc68eecd3ea69bc   \n",
       "3  db23ee8f56ac475f945bb645861074f4   \n",
       "4  95e66b8a95e54ccaa2c259d3abdcc864   \n",
       "\n",
       "                                      corrected_text  \\\n",
       "0  this was me and i have not edited wikipedia in...   \n",
       "1                person like how said we all hetty b   \n",
       "2  postscript and this article is still really pa...   \n",
       "3  the current infobox of this section with repre...   \n",
       "4  i was not born lastnight i know these hoes is ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [this, be, me, and, i, have, not, edit, wikipe...  \n",
       "1        [person, like, how, say, we, all, hetty, b]  \n",
       "2  [postscript, and, this, article, be, still, re...  \n",
       "3  [the, current, <UNK>, of, this, section, with,...  \n",
       "4  [i, be, not, bear, <UNK>, i, know, these, hoe,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 18:28:15.795338: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-01-24 18:28:15.795472: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-01-24 18:28:15.796211: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-01-24 18:28:15.796872: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-24 18:28:15.797467: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from utils.custom_metrics import RecallMulticlass, PrecisionMulticlass, F1ScoreMulticlass, WeightedCategoricalCrossEntropy\n",
    "\n",
    "# metrics\n",
    "metrics = [RecallMulticlass(name=\"recall\", n_class=2), \n",
    "           PrecisionMulticlass(name=\"precision\", n_class=2), \n",
    "           F1ScoreMulticlass(name=\"f1\", n_class=2)]\n",
    "\n",
    "# weights\n",
    "weights = data[\"label\"].value_counts(normalize=True).sort_index().values\n",
    "weights = 1/weights\n",
    "weights = weights/weights.sum()\n",
    "\n",
    "# loss\n",
    "loss = WeightedCategoricalCrossEntropy(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theopinto--dalle/code/arewetoxic/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105402,) (11712,) (105402, 2) (11712, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X, y = data[\"corrected_text\"], data[\"label\"]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# Tokenize\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the data\n",
    "def encode_data(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Encode the training data\n",
    "encoded_data = encode_data(X_train)\n",
    "encoded_val_data = encode_data(X_test)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = tf.convert_to_tensor(y_train)\n",
    "val_labels = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train class distribution: [0.6660215  0.33397847]\n",
      "y_test class distribution: [0.66282445 0.33717555]\n"
     ]
    }
   ],
   "source": [
    "# Investigate class distribution in y_train and y_test\n",
    "print(f\"y_train class distribution: {tf.divide(tf.reduce_sum(labels, axis=0), tf.reduce_sum(labels))}\")\n",
    "print(f\"y_test class distribution: {tf.divide(tf.reduce_sum(val_labels, axis=0), tf.reduce_sum(val_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(encoded_data),\n",
    "    labels\n",
    "))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(encoded_val_data),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "# Batch the dataset\n",
    "batch_size = 128\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "validation_dataset = validation_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Early stopping\n",
    "\n",
    "def early_stopping():\n",
    "    return tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_recall',     \n",
    "        patience=10,             \n",
    "        mode='max',            \n",
    "        min_delta=0.001,        \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "# TensorBoard\n",
    "#def tensorboard(log_dir:str = os.path.join(project_root, \"logs\", \"fit\")):\n",
    "#    return tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# ModelCheckpoint\n",
    "def model_checkpoint(model_name):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(project_root, \"models\", \"bert\", f\"{model_name}\"),\n",
    "        monitor='val_recall',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        save_format='tf',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [early_stopping(), model_checkpoint(\"bert_model_test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   40         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  98432     ['tf_bert_model_1[0][1]']     \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)        (None, 128)                  0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 64)                   8256      ['dropout_75[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)        (None, 64)                   0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 32)                   2080      ['dropout_76[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 2)                    66        ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109591074 (418.06 MB)\n",
      "Trainable params: 108834 (425.13 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_bert_model(loss: list, metrics: list, name:str = \"bert_model\"):\n",
    "    # Load the pre-trained BERT model\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Freeze the BERT model layers\n",
    "    for layer in bert_model.layers:  # Freeze all layers\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define the input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    # Get the output from the BERT model\n",
    "    bert_outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Use the pooled output for classification\n",
    "    pooled_output = bert_outputs.pooler_output\n",
    "\n",
    "    # Add custom layers\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output, name=name)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_bert_model(loss, metrics, \"bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - ETA: 0s - loss: 0.5131 - recall: 0.5423 - precision: 0.5439 - f1: 0.5431\n",
      "Epoch 1: val_recall improved from -inf to 0.51188, saving model to /Users/theopinto--dalle/code/arewetoxic/models/bert/bert_model_test\n",
      "3/3 [==============================] - 324s 160s/step - loss: 0.5131 - recall: 0.5423 - precision: 0.5439 - f1: 0.5431 - val_loss: 0.4186 - val_recall: 0.5119 - val_precision: 0.6459 - val_f1: 0.5711\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(training_dataset, epochs=1, callbacks=callbacks, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_weights(os.path.join(project_root, \"models\", \"bert_dummy.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
