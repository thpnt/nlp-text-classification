{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(\"..\")  # Adds the parent directory to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss\n",
    "weights = pd.Series([0.104978, 0.328745,0.566277])\n",
    "class WeightedCategoricalCrossEntropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, weights=weights, name='weighted_categorical_crossentropy', **kwargs):\n",
    "        super(WeightedCategoricalCrossEntropy, self).__init__()\n",
    "        self.weights = tf.cast(weights, tf.float32)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Clip y_pred to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        weighted_losses = -self.weights * y_true * tf.math.log(y_pred)\n",
    "        return tf.reduce_mean(tf.reduce_sum(weighted_losses, axis=1))\n",
    "    \n",
    "    \n",
    "# Define custom metrics\n",
    "class PrecisionMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='precision', n_class=3, **kwargs):\n",
    "        super(PrecisionMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.precision = self.add_weight(\n",
    "            shape=(n_class,),\n",
    "            name='precision',\n",
    "            initializer='zeros')\n",
    "        self.n_class = n_class\n",
    "        self.true_positives = self.add_weight(name='true_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='false_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        \n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        y_pred = tf.cast(tf.one_hot(tf.argmax(y_pred, axis=1), self.n_class), tf.int64)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            true_positive = tf.reduce_sum(y_true[:, i] * y_pred[:, i])\n",
    "            false_positive = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true[:, i], 0), tf.equal(y_pred[:, i], 1)), tf.int64))\n",
    "            \n",
    "            index = [[i]]  # Index for the class we are updating\n",
    "            self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, index, [true_positive]))\n",
    "            self.false_positives.assign(tf.tensor_scatter_nd_add(self.false_positives, index, [false_positive]))\n",
    "            \n",
    "    def result(self):\n",
    "        precision_per_class = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(precision_per_class)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros(self.n_class))\n",
    "        self.false_positives.assign(tf.zeros(self.n_class))\n",
    "        \n",
    "        \n",
    "\n",
    "class RecallMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='recall', n_class=3, **kwargs):\n",
    "        super(RecallMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.recall = self.add_weight(\n",
    "            shape=(n_class,),\n",
    "            name='recall',\n",
    "            initializer='zeros')\n",
    "        self.n_class = n_class\n",
    "        self.true_positives = self.add_weight(name='true_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='false_negatives', shape=(self.n_class,), initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        y_pred = tf.cast(tf.one_hot(tf.argmax(y_pred, axis=1), self.n_class), tf.int64)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            true_positive = tf.reduce_sum(y_true[:, i] * y_pred[:, i])\n",
    "            false_negative = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true[:, i], 1), tf.equal(y_pred[:, i], 0)), tf.int64))\n",
    "            \n",
    "            index = [[i]]  # Index for the class we are updating\n",
    "            self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, index, [true_positive]))\n",
    "            self.false_negatives.assign(tf.tensor_scatter_nd_add(self.false_negatives, index, [false_negative]))\n",
    "            \n",
    "    def result(self):\n",
    "        recall_per_class = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(recall_per_class)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros(self.n_class))\n",
    "        self.false_negatives.assign(tf.zeros(self.n_class))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 300)         8861400   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, None, 128)         186880    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, 64)                49408     \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 16)                1040      \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9102939 (34.72 MB)\n",
      "Trainable params: 241539 (943.51 KB)\n",
      "Non-trainable params: 8861400 (33.80 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(os.path.join(project_root, 'models', 'bi_lstm_model'), custom_objects={\"WeightedCategoricalCrossEntropy\": WeightedCategoricalCrossEntropy,\n",
    "                                                                                                                 \"PrecisionMulticlass\": PrecisionMulticlass,\n",
    "                                                                                                                 \"RecallMulticlass\": RecallMulticlass})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to predict from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"However this sentence is very insulting you should go fuck yourself bitch this is a test sentence just to see if the model works but you still are a dumb ass !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from utils.artifacts import REGEX_REMOVE, REGEX_REPLACE\n",
    "from textblob import TextBlob\n",
    "import signal\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet, words, webtext, gutenberg, brown\n",
    "from utils.cleaning import process_batch\n",
    "from utils.artifacts import slang_dict\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('webtext', quiet=True)\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "stop_words = stopwords.words('english')\n",
    "combined_corpus = set(words.words()) | set(wordnet.words()) | set(webtext.words()) | set(gutenberg.words()) | set(brown.words())\n",
    "combined_corpus = {word.lower() for word in combined_corpus}\n",
    "\n",
    "def clean_data(batch, stop_words = stop_words, slang_dict=slang_dict):\n",
    "        def clean_text(text: str) -> str:\n",
    "            # Apply REGEX_REMOVE and REGEX_REPLACE\n",
    "            for pattern in REGEX_REMOVE:\n",
    "                text = re.sub(pattern, \"\", text)\n",
    "            for pattern, repl in REGEX_REPLACE.items():\n",
    "                text = re.sub(pattern, repl, text)\n",
    "            \n",
    "            # Apply additionnal text cleaning steps\n",
    "            text = re.sub(r'^RT @\\w+: ', '', text)\n",
    "            text = re.sub(r'http\\S+', ' ', text)\n",
    "            text = re.sub(r'\\b\\w*jpeg\\w*\\b|\\b\\w*jpg\\w*\\b', '', text)\n",
    "            text = re.sub(r'\\n', ' ', text)\n",
    "            text = re.sub(r'@\\w+', '<PERSON>', text)\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            text = re.sub(r'\\b(\\w+)\\b\\s+\\1\\b', '', text)\n",
    "            text = text.strip().lower()\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "            text = re.sub(r'[\\x80-\\xFF]', '', text)\n",
    "            return text\n",
    "\n",
    "        \n",
    "        def correct_text(text: str, stop_words, slang_dict: dict) -> str:\n",
    "            tokens = text.split()\n",
    "            tokens = [slang_dict.get(word, word) for word in tokens]\n",
    "            #tokens = [word for word in tokens if word not in stop_words]\n",
    "            tokens = [word for word in tokens if len(word) < 15]\n",
    "            text = ' '.join(tokens)\n",
    "            corrected_text = str(TextBlob(text).correct())\n",
    "            return corrected_text\n",
    "        \n",
    "        \n",
    "        def lemma_text(tokens: list) -> list:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            ls = [lemmatizer.lemmatize(token, 'v') for token in tokens]\n",
    "            ls = [lemmatizer.lemmatize(token, 'n') for token in ls]\n",
    "            ls = [lemmatizer.lemmatize(token, 'a') for token in ls]\n",
    "            return ls\n",
    "        \n",
    "        def replace_unknown_tokens(tokens: list) -> list:\n",
    "            return [token if token in combined_corpus else '<UNK>' for token in tokens]\n",
    "        \n",
    "        \n",
    "        def combined_cleaning(text: str) -> list:\n",
    "            text = clean_text(text)\n",
    "            corrected_text = correct_text(text, stop_words, slang_dict)\n",
    "            return corrected_text\n",
    "        \n",
    "        def tokenize(text: str) -> list:\n",
    "            tokens = word_tokenize(text, preserve_line=True)\n",
    "            tokens = lemma_text(tokens)\n",
    "            tokens = replace_unknown_tokens(tokens)\n",
    "            return tokens\n",
    "        \n",
    "        # Process each text in the batch\n",
    "        batch['corrected_text'] = batch['text'].apply(combined_cleaning)\n",
    "        batch['tokens'] = batch['corrected_text'].apply(tokenize)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corrected_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However this sentence is very insulting you sh...</td>\n",
       "      <td>however this sentence is very insulting you sh...</td>\n",
       "      <td>[however, this, sentence, be, very, insult, yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  However this sentence is very insulting you sh...   \n",
       "\n",
       "                                      corrected_text  \\\n",
       "0  however this sentence is very insulting you sh...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [however, this, sentence, be, very, insult, yo...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame([input_text], columns=['text'])\n",
    "\n",
    "# Clean the text\n",
    "cleaned_data = clean_data(dataframe, stop_words, slang_dict)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def feature_engineering(data, max_length:int=170, vocab_size:int=29538):\n",
    "\n",
    "data = cleaned_data.copy()\n",
    "\n",
    "# Vocabulary size and vocabulary list for embedding)\n",
    "embedding_matrix = np.load(os.path.join(project_root, 'models', 'embedding_matrix_300.npy'))\n",
    "vocab = list(np.load(os.path.join(project_root, 'models', 'vocab.npy'), allow_pickle=True))\n",
    "vocab.append(\"<PAD>\")\n",
    "vocab_size = len(vocab)\n",
    "# Token to index\n",
    "token_to_index = {token: idx for idx, token in enumerate(vocab)}\n",
    "# Convert text tokens to index\n",
    "data['tokens_index'] = data['tokens'].apply(lambda x: [token_to_index.get(token, token_to_index[\"<UNK>\"]) for token in x])\n",
    "\n",
    "\n",
    "# Pad sequences\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "max_length = 170\n",
    "\n",
    "pad_sequences = pad_sequences(data.tokens_index, maxlen=max_length, padding='post', truncating='post')\n",
    "data['padded_tokens'] = [list(row) for row in pad_sequences]\n",
    "\n",
    "# Convert to tensor for inference\n",
    "data_tensor = tf.ragged.constant(data[\"padded_tokens\"], dtype=tf.int32)\n",
    "data_tensor = data_tensor.to_tensor(default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.98801464, 0.0088652 , 0.00312017]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
