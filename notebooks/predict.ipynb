{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(\"..\")  # Adds the parent directory to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss\n",
    "weights = pd.Series([0.104978, 0.328745,0.566277])\n",
    "class WeightedCategoricalCrossEntropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, weights=weights, name='weighted_categorical_crossentropy', **kwargs):\n",
    "        super(WeightedCategoricalCrossEntropy, self).__init__()\n",
    "        self.weights = tf.cast(weights, tf.float32)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Clip y_pred to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        weighted_losses = -self.weights * y_true * tf.math.log(y_pred)\n",
    "        return tf.reduce_mean(tf.reduce_sum(weighted_losses, axis=1))\n",
    "    \n",
    "    \n",
    "# Define custom metrics\n",
    "class PrecisionMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='precision', n_class=3, **kwargs):\n",
    "        super(PrecisionMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.precision = self.add_weight(\n",
    "            shape=(n_class,),\n",
    "            name='precision',\n",
    "            initializer='zeros')\n",
    "        self.n_class = n_class\n",
    "        self.true_positives = self.add_weight(name='true_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='false_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        \n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        y_pred = tf.cast(tf.one_hot(tf.argmax(y_pred, axis=1), self.n_class), tf.int64)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            true_positive = tf.reduce_sum(y_true[:, i] * y_pred[:, i])\n",
    "            false_positive = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true[:, i], 0), tf.equal(y_pred[:, i], 1)), tf.int64))\n",
    "            \n",
    "            index = [[i]]  # Index for the class we are updating\n",
    "            self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, index, [true_positive]))\n",
    "            self.false_positives.assign(tf.tensor_scatter_nd_add(self.false_positives, index, [false_positive]))\n",
    "            \n",
    "    def result(self):\n",
    "        precision_per_class = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(precision_per_class)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros(self.n_class))\n",
    "        self.false_positives.assign(tf.zeros(self.n_class))\n",
    "        \n",
    "        \n",
    "\n",
    "class RecallMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='recall', n_class=3, **kwargs):\n",
    "        super(RecallMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.recall = self.add_weight(\n",
    "            shape=(n_class,),\n",
    "            name='recall',\n",
    "            initializer='zeros')\n",
    "        self.n_class = n_class\n",
    "        self.true_positives = self.add_weight(name='true_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='false_negatives', shape=(self.n_class,), initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        y_pred = tf.cast(tf.one_hot(tf.argmax(y_pred, axis=1), self.n_class), tf.int64)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            true_positive = tf.reduce_sum(y_true[:, i] * y_pred[:, i])\n",
    "            false_negative = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true[:, i], 1), tf.equal(y_pred[:, i], 0)), tf.int64))\n",
    "            \n",
    "            index = [[i]]  # Index for the class we are updating\n",
    "            self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, index, [true_positive]))\n",
    "            self.false_negatives.assign(tf.tensor_scatter_nd_add(self.false_negatives, index, [false_negative]))\n",
    "            \n",
    "    def result(self):\n",
    "        recall_per_class = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(recall_per_class)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros(self.n_class))\n",
    "        self.false_negatives.assign(tf.zeros(self.n_class))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 300)         8861400   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, None, 128)         186880    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, 64)                49408     \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 16)                1040      \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9102939 (34.72 MB)\n",
      "Trainable params: 241539 (943.51 KB)\n",
      "Non-trainable params: 8861400 (33.80 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(os.path.join(project_root, 'models', 'bi_lstm_model'), custom_objects={\"WeightedCategoricalCrossEntropy\": WeightedCategoricalCrossEntropy,\n",
    "                                                                                                                 \"PrecisionMulticlass\": PrecisionMulticlass,\n",
    "                                                                                                                 \"RecallMulticlass\": RecallMulticlass})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to predict from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"However this sentence is very insulting you should go fuck yourself bitch this is a test sentence just to see if the model works but you still are a dumb ass !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from utils.artifacts import REGEX_REMOVE, REGEX_REPLACE\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import signal\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet, words, webtext, gutenberg, brown\n",
    "from utils.artifacts import slang_dict\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('webtext', quiet=True)\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "stop_words = stopwords.words('english')\n",
    "combined_corpus = set(words.words()) | set(wordnet.words()) | set(webtext.words()) | set(gutenberg.words()) | set(brown.words())\n",
    "combined_corpus = {word.lower() for word in combined_corpus}\n",
    "\n",
    "\n",
    "def clean_data(batch, stop_words = stop_words, slang_dict=slang_dict):\n",
    "        def clean_text(text: str) -> str:\n",
    "            # Apply REGEX_REMOVE and REGEX_REPLACE\n",
    "            for pattern in REGEX_REMOVE:\n",
    "                text = re.sub(pattern, \"\", text)\n",
    "            for pattern, repl in REGEX_REPLACE.items():\n",
    "                text = re.sub(pattern, repl, text)\n",
    "            \n",
    "            # Apply additionnal text cleaning steps\n",
    "            text = re.sub(r'^RT @\\w+: ', '', text)\n",
    "            text = re.sub(r'http\\S+', ' ', text)\n",
    "            text = re.sub(r'\\b\\w*jpeg\\w*\\b|\\b\\w*jpg\\w*\\b', '', text)\n",
    "            text = re.sub(r'\\n', ' ', text)\n",
    "            text = re.sub(r'@\\w+', '<PERSON>', text)\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            text = re.sub(r'\\b(\\w+)\\b\\s+\\1\\b', '', text)\n",
    "            text = text.strip().lower()\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "            text = re.sub(r'[\\x80-\\xFF]', '', text)\n",
    "            return text\n",
    "\n",
    "        \n",
    "        def correct_text(text: str, stop_words, slang_dict: dict) -> str:\n",
    "            tokens = text.split()\n",
    "            tokens = [slang_dict.get(word, word) for word in tokens]\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "            #tokens = [word for word in tokens if len(word) < 15]\n",
    "            text = ' '.join(tokens)\n",
    "            corrected_text = str(TextBlob(text).correct())\n",
    "            return corrected_text\n",
    "        \n",
    "        \n",
    "        #def lemma_text(tokens: list) -> list:\n",
    "        #    lemmatizer = WordNetLemmatizer()\n",
    "        #    \n",
    "        #    ls = [lemmatizer.lemmatize(token, 'v') for token in tokens]\n",
    "        #    ls = [lemmatizer.lemmatize(token, 'n') for token in ls]\n",
    "        #    ls = [lemmatizer.lemmatize(token, 'a') for token in ls]\n",
    "        #    return ls\n",
    "        \n",
    "        def replace_unknown_tokens(tokens: list) -> list:\n",
    "            return [token if token in combined_corpus else '<UNK>' for token in tokens]\n",
    "        \n",
    "        \n",
    "        def combined_cleaning(text: str) -> list:\n",
    "            text = clean_text(text)\n",
    "            corrected_text = correct_text(text, stop_words, slang_dict)\n",
    "            return corrected_text\n",
    "        \n",
    "        #def tokenize(text: str) -> list:\n",
    "        #    tokens = word_tokenize(text, preserve_line=True)\n",
    "        #    tokens = lemma_text(tokens)\n",
    "        #    tokens = replace_unknown_tokens(tokens)\n",
    "        #    return tokens\n",
    "        \n",
    "        # Process each text in the batch\n",
    "        batch['corrected_text'] = batch['text'].apply(combined_cleaning)\n",
    "        #batch['tokens'] = batch['corrected_text'].apply(tokenize)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corrected_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However this sentence is very insulting you sh...</td>\n",
       "      <td>however this sentence is very insulting you sh...</td>\n",
       "      <td>[however, this, sentence, be, very, insult, yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  However this sentence is very insulting you sh...   \n",
       "\n",
       "                                      corrected_text  \\\n",
       "0  however this sentence is very insulting you sh...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [however, this, sentence, be, very, insult, yo...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame([input_text], columns=['text'])\n",
    "\n",
    "# Clean the text\n",
    "cleaned_data = clean_data(dataframe, stop_words, slang_dict)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def feature_engineering(data, max_length:int=170, vocab_size:int=29538):\n",
    "\n",
    "data = cleaned_data.copy()\n",
    "\n",
    "# Vocabulary size and vocabulary list for embedding)\n",
    "embedding_matrix = np.load(os.path.join(project_root, 'models', 'embedding_matrix_300.npy'))\n",
    "vocab = list(np.load(os.path.join(project_root, 'models', 'vocab.npy'), allow_pickle=True))\n",
    "vocab.append(\"<PAD>\")\n",
    "vocab_size = len(vocab)\n",
    "# Token to index\n",
    "token_to_index = {token: idx for idx, token in enumerate(vocab)}\n",
    "# Convert text tokens to index\n",
    "data['tokens_index'] = data['tokens'].apply(lambda x: [token_to_index.get(token, token_to_index[\"<UNK>\"]) for token in x])\n",
    "\n",
    "\n",
    "# Pad sequences\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "max_length = 170\n",
    "\n",
    "pad_sequences = pad_sequences(data.tokens_index, maxlen=max_length, padding='post', truncating='post')\n",
    "data['padded_tokens'] = [list(row) for row in pad_sequences]\n",
    "\n",
    "# Convert to tensor for inference\n",
    "data_tensor = tf.ragged.constant(data[\"padded_tokens\"], dtype=tf.int32)\n",
    "data_tensor = data_tensor.to_tensor(default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.98801464, 0.0088652 , 0.00312017]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 09:08:39.468465: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-11-29 09:08:39.468516: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-11-29 09:08:39.468534: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-11-29 09:08:39.468751: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-29 09:08:39.469137: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-11-29 09:08:41.478285: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "from utils.custom_metrics import (\n",
    "    WeightedCategoricalCrossEntropy, \n",
    "    PrecisionMulticlass, \n",
    "    RecallMulticlass,\n",
    "    F1ScoreMulticlass,\n",
    "    weights\n",
    ")\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    os.path.join(project_root, \"models\", \"bi_gru\"),\n",
    "    custom_objects={'PrecisionMulticlass': PrecisionMulticlass,\n",
    "                    'RecallMulticlass': RecallMulticlass,\n",
    "                    'F1ScoreMulticlass': F1ScoreMulticlass,\n",
    "                    'WeightedCategoricalCrossEntropy': partial(WeightedCategoricalCrossEntropy, weights=weights)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I a.\"\n",
    "\n",
    "\n",
    "def build_predict_dataset(data: pd.DataFrame, batch_size:int = 512):\n",
    "    # Prepare dataset\n",
    "    X = data[\"text\"]\n",
    "\n",
    "    # Create tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\"text\": [user_input]})\n",
    "data = clean_data(data)\n",
    "data = build_predict_dataset(data).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0310078, 0.9689922]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Adds the parent directory to the path\n",
    "project_root = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pickle\n",
    "path = os.path.join(project_root, \"datasets/processed/data.pkl\")\n",
    "data = pd.read_pickle(path)\n",
    "data['label'] = data['label'].apply(lambda x: 1 if x == 2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_3 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   40         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 128)                  98432     ['tf_bert_model_3[0][1]']     \n",
      "                                                                                                  \n",
      " dropout_151 (Dropout)       (None, 128)                  0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 64)                   8256      ['dropout_151[0][0]']         \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 2)                    130       ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109589058 (418.05 MB)\n",
      "Trainable params: 106818 (417.26 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x368a168f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "from utils.custom_metrics import RecallMulticlass, PrecisionMulticlass, F1ScoreMulticlass, WeightedCategoricalCrossEntropy\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "metrics = [RecallMulticlass(name=\"recall\", n_class=2), PrecisionMulticlass(name=\"precision\", n_class=2), F1ScoreMulticlass(name=\"f1\", n_class=2)]\n",
    "\n",
    "# weights\n",
    "weights = data[\"label\"].value_counts(normalize=True).sort_index().values\n",
    "weights = 1/weights\n",
    "weights = weights/weights.sum()\n",
    "\n",
    "# loss\n",
    "loss = WeightedCategoricalCrossEntropy(weights)\n",
    "\n",
    "\n",
    "def build_bert_model(loss: list, metrics: list, name:str = \"bert_model\"):\n",
    "    # Load the pre-trained BERT model\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Freeze the BERT model layers\n",
    "    for layer in bert_model.layers:  # Freeze all layers\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define the input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    # Get the output from the BERT model\n",
    "    bert_outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Use the pooled output for classification\n",
    "    pooled_output = bert_outputs.pooler_output\n",
    "\n",
    "    # Add custom layers\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output, name=name)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "test_model = build_bert_model(loss, metrics, \"bert_model\")\n",
    "# Load the weights\n",
    "test_model.load_weights(os.path.join(project_root, \"models\", \"bert\", \"bert_model_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theopinto--dalle/code/arewetoxic/env/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "2024-12-02 23:54:14.410689: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n"
     ]
    }
   ],
   "source": [
    "def build_predict_dataset(data: pd.DataFrame, tokenizer, max_length=128, batch_size=512):\n",
    "    # Prepare dataset\n",
    "    texts = data[\"text\"].tolist()\n",
    "\n",
    "    # Tokenize and encode the data\n",
    "    encoded_data = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    # Create tf.data.Dataset with tokenized inputs\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encoded_data)  # This ensures both input_ids and attention_mask are included\n",
    "    ))\n",
    "\n",
    "    # Batch the dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "# Tokenizer initialization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = [\n",
    "    \"Hi Team, I think there are some areas where we can improve this proposal to align with the client's goals. Let's discuss during the next meeting.\",\n",
    "    \"People often misjudge others without knowing their full story. Let's try to be more understanding and empathetic.\",\n",
    "    \"Hey, I saw your post earlier. It made me think a lot about how we treat each other in difficult times. Hope you're doing well!\",\n",
    "    \"This video really opened my eyes to the challenges faced by different communities. Thank you for sharing this perspective.\",\n",
    "    \"Good progress on the project so far! Let's make sure we meet the deadline by keeping communication open and addressing issues promptly.\",\n",
    "    \"Sometimes it's not about agreeing with everyone but about respecting their right to a different opinion. Stay kind.\",\n",
    "    \"I just wanted to say that I really appreciate the way you handled the discussion yesterday. You kept it professional and constructive.\",\n",
    "    \"Great tutorial! I didn't understand this concept before, but your explanation made it so much clearer. Subscribed!\",\n",
    "    \"Not every problem has an easy answer, but working together gets us closer to a solution. Keep going, everyone!\",\n",
    "    \"Hi Sarah, I just wanted to thank you for your support during the project. It really made a difference, and I'm grateful for your help.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = [\n",
    "    \"You are so incompetent, it's embarrassing to work with someone like you.\",\n",
    "    \"What a joke of a person. I can't believe anyone would take you seriously.\",\n",
    "    \"Nobody cares about your opinion, so just shut up and go away.\",\n",
    "    \"You're clearly too dumb to understand basic things. Maybe stay quiet next time.\",\n",
    "    \"People like you are the reason this country is going downhill.\",\n",
    "    \"How delusional do you have to be to post something this idiotic?\",\n",
    "    \"Your presence here is not just unwelcome, it's downright disgusting.\",\n",
    "    \"Seriously, just quit already. You're the most useless person in the team.\",\n",
    "    \"Why don't you go back to whatever rock you crawled out from?\",\n",
    "    \"It's pathetic how you keep trying and still fail every single time.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = [\n",
    "    \"You're such a loser, nobody cares about you here.\",\n",
    "    \"Why are you even posting? No one wants to see your stupid opinions.\",\n",
    "    \"You look so dumb in that picture, I can't believe you uploaded it.\",\n",
    "    \"Nobody likes you, you're just a burden to everyone.\",\n",
    "    \"You're so annoying, why don't you just leave everyone alone?\",\n",
    "    \"Seriously, how are you even still friends with them? They must feel sorry for you.\",\n",
    "    \"Why don't you just stop trying? You'll never be as good as them.\",\n",
    "    \"You think you're so special, but you're just like everyone else – useless.\",\n",
    "    \"You're so pathetic, no wonder everyone ignores you.\",\n",
    "    \"Why do you always have to act like you're so smart? You're really not.\",\n",
    "    \"You're just a kid. What do you even know? Your opinion doesn't matter.\",\n",
    "    \"Oh wow, you're so brave talking behind a screen. Try saying it to my face.\",\n",
    "    \"I feel sorry for you, but I guess that's what you get for being such a weirdo.\",\n",
    "    \"Do you honestly think anyone cares about your life? Get over yourself.\",\n",
    "    \"You're just a waste of space, no one's going to remember you.\",\n",
    "    \"I wouldn't be surprised if you ended up alone forever, no one likes people like you.\",\n",
    "    \"You think you're cool, but you're just embarrassing yourself.\",\n",
    "    \"I can't believe you're still hanging out with them, they're only tolerating you.\",\n",
    "    \"Why don't you just quit? You'll never make it anywhere.\",\n",
    "    \"You look like you don't even belong here, just leave.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 640ms/step\n",
      "[[0.6698157  0.33018425]\n",
      " [0.38523373 0.6147663 ]\n",
      " [0.09626018 0.9037398 ]\n",
      " [0.6853213  0.3146787 ]\n",
      " [0.4165259  0.5834741 ]\n",
      " [0.26706153 0.73293847]\n",
      " [0.6174586  0.3825414 ]\n",
      " [0.32340345 0.6765966 ]\n",
      " [0.045049   0.95495105]\n",
      " [0.25105333 0.74894667]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\"text\": user_input})\n",
    "\n",
    "\n",
    "# Clean the data (if you have a cleaning function defined)\n",
    "data = clean_data(data)\n",
    "\n",
    "# Build the prediction dataset\n",
    "predict_dataset = build_predict_dataset(data, tokenizer)\n",
    "\n",
    "# Make predictions\n",
    "predictions = test_model.predict(predict_dataset)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   40         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 128)                  98432     ['tf_bert_model_4[0][1]']     \n",
      "                                                                                                  \n",
      " dropout_189 (Dropout)       (None, 128)                  0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 64)                   8256      ['dropout_189[0][0]']         \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 2)                    130       ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109589058 (418.05 MB)\n",
      "Trainable params: 106818 (417.26 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x384be0dc0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= build_bert_model(loss, metrics, \"bert_model\")\n",
    "# Load the weights\n",
    "model.load_weights(os.path.join(project_root, \"models\", \"bert\", \"bert_model_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.17936371 0.8206363 ]\n",
      " [0.15537009 0.8446299 ]\n",
      " [0.03791552 0.9620845 ]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\"text\": user_input})\n",
    "\n",
    "\n",
    "# Clean the data (if you have a cleaning function defined)\n",
    "data = clean_data(data)\n",
    "\n",
    "# Build the prediction dataset\n",
    "predict_dataset = build_predict_dataset(data, tokenizer)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(predict_dataset)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
