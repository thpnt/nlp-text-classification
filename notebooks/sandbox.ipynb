{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Params\n",
    "vocab = list(set([token for token in data['tokens'] for token in token]))\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metrics = [\n",
    "      RecallMulticlass(name='recall', n_class=train_dataset.element_spec[1].shape[0]),\n",
    "      PrecisionMulticlass(name='precision', n_class=train_dataset.element_spec[1].shape[0])\n",
    "      ]\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    name='embedding'\n",
    ")\n",
    "\n",
    "# Model\n",
    "baseline_model = Sequential([\n",
    "    embedding_layer,\n",
    "    LSTM(units=64, name=\"lstm1\"),\n",
    "    Dense(units=64, activation='relu', name='dense1'),\n",
    "    Dense(units=3, activation='softmax', name='output')\n",
    "], name='baseline_model')\n",
    "\n",
    "\n",
    "baseline_model.compile(optimizer='adam', loss=WeightedCategoricalCrossEntropy(weights=weights),\n",
    "                       metrics=metrics)\n",
    "\n",
    "baseline_model.summary()\n",
    "\n",
    "# train the model\n",
    "baseline_model_history = baseline_model.fit(batched_dataset, epochs=10, callbacks=[early_stopping, tensorboard_callback],\n",
    "                                            validation_data=batched_evaluation)\n",
    "\n",
    "# Evaluate the model\n",
    "#baseline_model.evaluate(batched_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = baseline_model.evaluate(batched_evaluation)\n",
    "eval_loss, eval_precision, eval_recall = eval_results  # Assuming three metrics\n",
    "eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.save(os.path.join(project_root, 'models', 'baseline_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "mlruns_path = os.path.join(project_root, 'mlruns')\n",
    "\n",
    "# Set the tracking URI to the custom path\n",
    "mlflow.set_tracking_uri(f\"file://{mlruns_path}\")\n",
    "\n",
    "## Create or get an existing experiment by name\n",
    "experiment_name = \"baseline_experiment\"\n",
    "#experiment_id = mlflow.create_experiment(experiment_name, artifact_location=f\"file://{mlruns_path}\")\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"baseline_model\") as run:\n",
    "    # Log model\n",
    "    mlflow.tensorflow.log_model(baseline_model, \"model\")\n",
    "    # Log metrics\n",
    "    \n",
    "    for epoch, metrics in enumerate(baseline_model_history.history[\"loss\"]):\n",
    "        mlflow.log_metric(\"loss\", metrics, step=epoch)\n",
    "        \n",
    "    for epoch, metrics in enumerate(baseline_model_history.history[\"recall\"]):\n",
    "        mlflow.log_metric(\"recall\", metrics, step=epoch)\n",
    "        \n",
    "    for epoch, metrics in enumerate(baseline_model_history.history[\"precision\"]):\n",
    "        mlflow.log_metric(\"precision\", metrics, step=epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def plot_metrics_with_seaborn(history):\n",
    "\n",
    "    # Extract metrics from history\n",
    "    metrics = [key for key in history.history.keys() if 'val_' not in key]\n",
    "    epochs = range(1, len(history.history[metrics[0]]) + 1)\n",
    "    \n",
    "\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        \n",
    "        # Plot training metric\n",
    "        sns.lineplot(x=epochs, y=history.history[metric], label=f'Training {metric.capitalize()}', marker='o')\n",
    "        \n",
    "        # Plot validation metric if available\n",
    "        if f'val_{metric}' in history.history:\n",
    "            sns.lineplot(x=epochs, y=history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}', marker='o')\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f'{metric.capitalize()} Over Epochs', fontsize=16)\n",
    "        plt.xlabel('Epochs', fontsize=14)\n",
    "        plt.ylabel(metric.capitalize(), fontsize=14)\n",
    "        plt.legend(loc='best', fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.xticks(epochs)  # Show each epoch number on x-axis\n",
    "        plt.tight_layout()  # Adjust layout for better fit\n",
    "        plt.show()\n",
    "\n",
    "plot_metrics_with_seaborn(baseline_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
