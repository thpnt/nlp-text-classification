{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model developement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data & Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "project_root = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from utils.gcp import load_data_from_gcs\n",
    "from google.auth import credentials\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "service_account = os.path.join(project_root, os.getenv(\"GCP_SERVICE_ACCOUNT\"))\n",
    "client = storage.Client.from_service_account_json(service_account)\n",
    "\n",
    "# Load data from GCS\n",
    "bucket_name = os.getenv(\"GCP_BUCKET_NAME\")\n",
    "file_name = os.getenv(\"GCP_DATA_PATH\")\n",
    "data = load_data_from_gcs(bucket_name, file_name, client)\n",
    "\n",
    "data[\"tokens\"] = data[\"tokens\"].apply(lambda x: eval(x))\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "# Shuffle data and reset_index\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'].apply(lambda x: 1 if x == 2 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_distribution = data[\"label\"].value_counts(normalize=True)\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Explore input length and uniqueness\n",
    "\n",
    "# Length analysis in prepeparation for padding\n",
    "data['tokens_length'] = data['tokens'].apply(lambda x: len(x))\n",
    "mean_length = data['tokens_length'].mean()\n",
    "quantiles_length = data['tokens_length'].quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "\n",
    "# Unique tokens per sample\n",
    "data[\"unique_tokens\"] = data['tokens'].apply(lambda x: len(set(x)))\n",
    "mean_unique_tokens = data['unique_tokens'].mean()\n",
    "quantiles_unique_tokens = data['unique_tokens'].quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "\n",
    "\n",
    "# Vocabulary analysis\n",
    "vocab_size = len(set([token for token in data['tokens'] for token in token]))\n",
    "\n",
    "\n",
    "# Top most frequent tokens\n",
    "token_count = {}\n",
    "for row in tqdm(data.tokens, desc=\"Progress\"):\n",
    "\tfor token in row :\n",
    "\t\tif token not in list(token_count.keys()):\n",
    "\t\t\ttoken_count[token] = 1\n",
    "\t\telse:\n",
    "\t\t\ttoken_count[token] += 1\n",
    "token_count = dict(sorted(token_count.items(), key=lambda item: item[1], reverse=True))\n",
    "top_tokens = dict(list(token_count.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data\n",
    "x = list(top_tokens.keys())\n",
    "y = list(top_tokens.values())\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.barplot(x=x, y=y, ax=ax)\n",
    "ax.set_title(\"Top 20 most frequent tokens\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Token\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length distribution and outlier detection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Apply log transformation\n",
    "data['log_tokens_length'] = data['tokens_length'].apply(lambda x: np.log(x+1))\n",
    "\n",
    "# Token length distribution\n",
    "sns.histplot(data['log_tokens_length'], kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"Token length distribution\")\n",
    "\n",
    "# Outlier detection\n",
    "sns.scatterplot(x=list(data.index), y='log_tokens_length', data=data, ax=ax[1])\n",
    "ax[1].set_title(\"Token length diagnostic plot\")\n",
    "\n",
    "# Outlier detection\n",
    "mean = data['log_tokens_length'].mean()\n",
    "median = data['log_tokens_length'].median()\n",
    "std = data['log_tokens_length'].std()\n",
    "mad = np.median(np.abs(data['log_tokens_length'] - median))\n",
    "\n",
    "upper_bound = median + 2.5*(1.48*mad), mean + 2.5*std\n",
    "lower_bound = median - 2.5*(1.48*mad), mean - 2.5*std\n",
    "\n",
    "ax[1].axhline(upper_bound[0], color='r', linestyle='--')\n",
    "ax[1].axhline(lower_bound[0], color='r', linestyle='--')\n",
    "ax[1].axhline(upper_bound[1], color='g', linestyle='--')\n",
    "ax[1].axhline(lower_bound[1], color='g', linestyle='--')\n",
    "ax[1].axhline(mean, color='g', linestyle='-')\n",
    "ax[1].axhline(median, color='r', linestyle='-')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore unique/length of tokens\n",
    "data[\"unique_ratio\"] = data['unique_tokens'] / data['tokens_length']\n",
    "\n",
    "# Apply log transformation\n",
    "epsilon = 1e-6\n",
    "data['log_unique_ratio'] = data['unique_ratio']\n",
    "\n",
    "\n",
    "# Unique ratio distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.histplot(data['unique_ratio'], kde=True, ax=ax)\n",
    "plt.title(\"Unique ratio distribution\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "data = data[(data['log_tokens_length'] < upper_bound[0]) & (data['log_tokens_length'] > lower_bound[0])]\n",
    "\n",
    "# Remove empty tokens\n",
    "data = data[data['tokens_length'] > 0]\n",
    "\n",
    "# Remove tokens with low unique ratio\n",
    "data = data[data['unique_ratio'] > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data locally\n",
    "import pickle\n",
    "path = os.path.join(project_root, \"datasets/processed/data.pkl\")\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Parameters for vectorization\n",
    "tokens = list(set([token for token in data['tokens'] for token in token]))  \n",
    "max_tokens = len(tokens) # Vocabulary size\n",
    "max_length = data.tokens_length.max()    # Sequence length after padding\n",
    "\n",
    "# Define TextVectorization layer\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,  # Maximum vocab size\n",
    "    output_mode=\"int\",      # Map tokens to integers\n",
    "    output_sequence_length=int(max_length), # Ensure padding/truncation\n",
    "    name=\"text_vectorizer\"  # Name of the layer\n",
    ")\n",
    "\n",
    "# Prepare and adapt the vectorizer to the dataset\n",
    "text_vectorizer.adapt(data[\"corrected_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format(os.path.join(project_root, 'datasets/.originals/Google-News-Vectors-Negative-300.bin'), binary=True)\n",
    "\n",
    "# Initialize the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(text_vectorizer.get_vocabulary()), embedding_dim))\n",
    "\n",
    "# Fill the embedding matrix with Word2Vec vectors\n",
    "for i, word in tqdm(enumerate(text_vectorizer.get_vocabulary()), desc='progression'):\n",
    "    if i == 0:  # Reserved for padding, indices are already zero-initialized\n",
    "        continue\n",
    "    elif word == \"[UNK]\":  # Default unknown token in TextVectorization\n",
    "        embedding_matrix[i] = np.random.rand(embedding_dim)\n",
    "    elif word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.rand(embedding_dim)  # Random vector for OOV tokens\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_dim = 300\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    name=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup (metrics, input, build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Define custom metrics\n",
    "class PrecisionMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='precision', n_class=2, **kwargs):\n",
    "        super(PrecisionMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.precision = self.add_weight(\n",
    "            shape=(n_class,),\n",
    "            name='precision',\n",
    "            initializer='zeros')\n",
    "        self.n_class = n_class\n",
    "        self.true_positives = self.add_weight(name='true_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='false_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        \n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        y_pred = tf.cast(tf.one_hot(tf.argmax(y_pred, axis=1), self.n_class), tf.int64)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            true_positive = tf.reduce_sum(y_true[:, i] * y_pred[:, i])\n",
    "            false_positive = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true[:, i], 0), tf.equal(y_pred[:, i], 1)), tf.int64))\n",
    "            \n",
    "            index = [[i]]  # Index for the class we are updating\n",
    "            self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, index, [true_positive]))\n",
    "            self.false_positives.assign(tf.tensor_scatter_nd_add(self.false_positives, index, [false_positive]))\n",
    "            \n",
    "    def result(self):\n",
    "        precision_per_class = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(precision_per_class)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros(self.n_class))\n",
    "        self.false_positives.assign(tf.zeros(self.n_class))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"n_class\": self.n_class,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "        \n",
    "\n",
    "class RecallMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='recall', n_class=2, **kwargs):\n",
    "        super(RecallMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.recall = self.add_weight(\n",
    "            shape=(n_class,),\n",
    "            name='recall',\n",
    "            initializer='zeros')\n",
    "        self.n_class = n_class\n",
    "        self.true_positives = self.add_weight(name='true_positives', shape=(self.n_class,), initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='false_negatives', shape=(self.n_class,), initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        y_pred = tf.cast(tf.one_hot(tf.argmax(y_pred, axis=1), self.n_class), tf.int64)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            true_positive = tf.reduce_sum(y_true[:, i] * y_pred[:, i])\n",
    "            false_negative = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true[:, i], 1), tf.equal(y_pred[:, i], 0)), tf.int64))\n",
    "            \n",
    "            index = [[i]]  # Index for the class we are updating\n",
    "            self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, index, [true_positive]))\n",
    "            self.false_negatives.assign(tf.tensor_scatter_nd_add(self.false_negatives, index, [false_negative]))\n",
    "            \n",
    "    def result(self):\n",
    "        recall_per_class = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(recall_per_class)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros(self.n_class))\n",
    "        self.false_negatives.assign(tf.zeros(self.n_class))\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"n_class\": self.n_class,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "        \n",
    "        \n",
    "        \n",
    "class F1ScoreMulticlass(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', n_class=2, **kwargs):\n",
    "        super(F1ScoreMulticlass, self).__init__(name=name, **kwargs)\n",
    "        self.n_class = n_class\n",
    "        self.precision = PrecisionMulticlass(n_class=n_class)\n",
    "        self.recall = RecallMulticlass(n_class=n_class)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        return f1_score\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"n_class\": self.n_class,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom weighted loss\n",
    "class_distribution = data['label'].value_counts(normalize=True)\n",
    "weights = (1/class_distribution) # Weight\n",
    "weights = weights / weights.sum() # Normalize\n",
    "\n",
    "# Define custom loss\n",
    "class WeightedCategoricalCrossEntropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, weights, name='weighted_categorical_crossentropy', **kwargs):\n",
    "        super(WeightedCategoricalCrossEntropy, self).__init__()\n",
    "        self.weights = tf.cast(weights, tf.float32)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Clip y_pred to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        weighted_losses = -self.weights * y_true * tf.math.log(y_pred)\n",
    "        return tf.reduce_mean(tf.reduce_sum(weighted_losses, axis=1))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weights\": self.weights,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Early stopping\n",
    "\n",
    "def early_stopping():\n",
    "    return tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='recall',     \n",
    "        patience=10,             \n",
    "        mode='max',            \n",
    "        min_delta=0.001,        \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "# TensorBoard\n",
    "def tensorboard(log_dir:str = os.path.join(project_root, \"logs\", \"fit\")):\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# ModelCheckpoint\n",
    "def model_checkpoint(model_name):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(project_root, \"models\", f\"{model_name}\"),\n",
    "        monitor='val_recall',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        save_format='tf',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data: pd.DataFrame, batch_size:int = 512):\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    \n",
    "    # Prepare dataset\n",
    "    X = data[\"corrected_text\"]\n",
    "    y = data[\"label\"]\n",
    "    y = to_categorical(y, num_classes=2)\n",
    "\n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create tf.data.Dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "\n",
    "    return train_dataset.batch(batch_size), test_dataset.batch(batch_size), X_test.index #, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Save the TextVectorization layer configuration and weights\n",
    "text_vectorizer_config = text_vectorizer.get_config()\n",
    "text_vectorizer_weights = text_vectorizer.get_weights()\n",
    "\n",
    "with open(os.path.join(project_root, 'models', 'text_vectorizer', 'config.pkl'), 'wb') as f:\n",
    "    pickle.dump(text_vectorizer_config, f)\n",
    "\n",
    "with open(os.path.join(project_root, 'models', 'text_vectorizer', 'weights.pkl'), 'wb') as f:\n",
    "    pickle.dump(text_vectorizer_weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test class balance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, X_train, X_test, y_train, y_test = build_dataset(data)\n",
    "\n",
    "# Inspect class balance on training set and test set\n",
    "train_labels = y_train.argmax(axis=1)\n",
    "test_labels = y_test.argmax(axis=1)\n",
    "\n",
    "# Train balance analysis\n",
    "class_count = np.unique(train_labels, return_counts=True)[1]\n",
    "total = class_count.sum()\n",
    "train_class_distribution = class_count / total\n",
    "print(f\"Train class distribution: {train_class_distribution}\")\n",
    "\n",
    "# Test balance analysis\n",
    "class_count = np.unique(test_labels, return_counts=True)[1]\n",
    "total = class_count.sum()\n",
    "test_class_distribution = class_count / total\n",
    "print(f\"Test class distribution: {test_class_distribution}\")\n",
    "\n",
    "# Total class distrib\n",
    "print(f\"Total class distribution : {list(class_distribution)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers: list, loss: tf.keras.losses.Loss, metrics: list, model_name: str):\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Define model\n",
    "    # Input Layer\n",
    "    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='features')\n",
    "    x = text_vectorizer(inputs)\n",
    "    x = embedding_layer(x)\n",
    "    \n",
    "    \n",
    "    # Hidden Layers\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "        \n",
    "    # Output Layer\n",
    "    outputs = tf.keras.layers.Dense(2, activation='softmax', name='output')(x)\n",
    "    \n",
    "    # Model\n",
    "    model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data, testing_data, epochs:int = 100, callbacks:list = None):\n",
    "    history = model.fit(training_data,\n",
    "        validation_data=testing_data,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def plot_classification_report(model, testing_data):\n",
    "    \"\"\"\n",
    "    This function takes a model and testing dataset as input, evaluates the model on the test set, \n",
    "    and generates a classification report with precision, recall, and F1 scores per class.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: A trained Keras model\n",
    "    - testing_data: A TensorFlow dataset (or a tf.data.Dataset) that contains features and labels for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "    - A bar plot with precision, recall, and F1 scores per label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    metrics = model.evaluate(testing_data)\n",
    "    # Print overall results\n",
    "    print(\"Overall results\")\n",
    "    for metric, value in zip(model.metrics_names, metrics):\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Get true labels and predictions\n",
    "    y_pred = model.predict(testing_data)\n",
    "    y_pred = y_pred.argmax(axis=1)  # Convert predictions to label indices (class labels)\n",
    "\n",
    "    # Extract all labels from the testing_data\n",
    "    def get_all_labels(dataset):\n",
    "        all_labels = []\n",
    "        for batch in dataset.as_numpy_iterator():\n",
    "            labels = batch[1]  # Assuming '1' corresponds to the labels in your dataset\n",
    "            all_labels.append(labels)\n",
    "        return np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Get true labels as a single NumPy array\n",
    "    true_labels_one_hot = get_all_labels(testing_data)\n",
    "    true_labels = true_labels_one_hot.argmax(axis=1)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, y_pred, target_names=[\"Neutral\", \"Toxic\"], output_dict=True)\n",
    "\n",
    "    # Convert the classification report dictionary into a pandas DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # Plotting grouped bars for precision, recall, and F1-score\n",
    "    ax = report_df[['precision', 'recall', 'f1-score']].plot(kind='barh', figsize=(10, 6))\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set_title('Classification Report Metrics per Label')\n",
    "    ax.set_xlabel('Scores')\n",
    "    ax.set_ylabel('Labels')\n",
    "    ax.legend(title=\"Metrics\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return ax, metrics  # Return axis object for further manipulation if needed\n",
    "\n",
    "\n",
    "\n",
    "def plot_metrics_with_seaborn(history):\n",
    "\n",
    "    # Extract metrics from history\n",
    "    metrics = [key for key in history.history.keys() if 'val_' not in key]\n",
    "    epochs = range(1, len(history.history[metrics[0]]) + 1)\n",
    "    \n",
    "\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        \n",
    "        # Plot training metric\n",
    "        sns.lineplot(x=epochs, y=history.history[metric], label=f'Training {metric.capitalize()}', marker='o')\n",
    "        \n",
    "        # Plot validation metric if available\n",
    "        if f'val_{metric}' in history.history:\n",
    "            sns.lineplot(x=epochs, y=history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}', marker='o')\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f'{metric.capitalize()} Over Epochs', fontsize=16)\n",
    "        plt.xlabel('Epochs', fontsize=14)\n",
    "        plt.ylabel(metric.capitalize(), fontsize=14)\n",
    "        plt.legend(loc='best', fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.xticks(epochs)  # Show each epoch number on x-axis\n",
    "        plt.tight_layout()  # Adjust layout for better fit\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPUs are available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "training_data, testing_data = build_dataset(data)\n",
    "\n",
    "# Metrics\n",
    "metrics = [\n",
    "    PrecisionMulticlass(n_class=2),\n",
    "    RecallMulticlass(n_class=2),\n",
    "    F1ScoreMulticlass(n_class=2),\n",
    "]\n",
    "\n",
    "# Loss\n",
    "loss = WeightedCategoricalCrossEntropy(weights)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    early_stopping(),\n",
    "    #tensorboard(),\n",
    "    model_checkpoint(\"baseline_model\")\n",
    "]\n",
    "\n",
    "# Layers\n",
    "layers = [\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False, name='lstm'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='dense_1'),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='dense_2')\n",
    "]\n",
    "\n",
    "# Model\n",
    "model = build_model(layers=layers, loss=loss, metrics=metrics, \n",
    "                    model_name=\"baseline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_history = train_model(model, training_data, testing_data, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_report(model, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_seaborn(baseline_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectionnal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "training_data, testing_data = build_dataset(data)\n",
    "\n",
    "# Metrics\n",
    "metrics = [\n",
    "    PrecisionMulticlass(n_class=2),\n",
    "    RecallMulticlass(n_class=2),\n",
    "    F1ScoreMulticlass(n_class=2),\n",
    "]\n",
    "\n",
    "# Loss\n",
    "loss = WeightedCategoricalCrossEntropy(weights)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    early_stopping(),\n",
    "    #tensorboard(),\n",
    "    model_checkpoint(\"bi_lstm\")\n",
    "]\n",
    "\n",
    "# Layers\n",
    "layers = [\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation=\"tanh\",return_sequences=True, name='bi_lstm')),\n",
    "    tf.keras.layers.Dropout(0.2, name='dropout'),\n",
    "    tf.keras.layers.LSTM(64, activation='tanh', name='lstm', return_sequences=False),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='dense_1'),\n",
    "    tf.keras.layers.Dropout(0.2, name='dropout_2'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='dense_2'),\n",
    "    tf.keras.layers.Dense(16, activation='relu', name='dense_3')\n",
    "]\n",
    "\n",
    "# Model\n",
    "bi_lstm_model = build_model(layers=layers, loss=loss, metrics=metrics, \n",
    "                    model_name=\"bi_lstm_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model_history = train_model(bi_lstm_model, training_data, testing_data, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_report(bi_lstm_model, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_seaborn(bi_lstm_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(os.path.join(project_root, \"datasets/processed/data.pkl\"))\n",
    "data['label'] = data['label'].apply(lambda x: 1 if x == 2 else x)\n",
    "training_data, testing_data, test_indices = build_dataset(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "#training_data, testing_data = build_dataset(data)\n",
    "\n",
    "# Metrics\n",
    "metrics = [\n",
    "    PrecisionMulticlass(n_class=2),\n",
    "    RecallMulticlass(n_class=2),\n",
    "    F1ScoreMulticlass(n_class=2),\n",
    "]\n",
    "\n",
    "# Loss\n",
    "loss = WeightedCategoricalCrossEntropy(weights)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    early_stopping(),\n",
    "    #tensorboard(),\n",
    "    model_checkpoint(\"bi_lstm\")\n",
    "]\n",
    "\n",
    "# Layers\n",
    "layers = [\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, activation=\"tanh\",return_sequences=True, name='bi_GRU')),\n",
    "    tf.keras.layers.Dropout(0.2, name='dropout'),\n",
    "    tf.keras.layers.GRU(128, activation='tanh', name='GRU'),\n",
    "    tf.keras.layers.Dropout(0.2, name='dropout_1'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='dense_1'),\n",
    "    tf.keras.layers.Dropout(0.2, name='dropout_2'),\n",
    "    tf.keras.layers.Dense(16, activation='relu', name='dense_2')\n",
    "]\n",
    "\n",
    "# Model\n",
    "bi_gru_model = build_model(layers=layers, loss=loss, metrics=metrics, \n",
    "                    model_name=\"bi_gru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gru_model_history = train_model(bi_gru_model, training_data, testing_data, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.custom_metrics import PrecisionMulticlass, RecallMulticlass, F1ScoreMulticlass, WeightedCategoricalCrossEntropy\n",
    "weights = pd.Series([0.334298, 0.665702])\n",
    "model = tf.keras.models.load_model(\n",
    "    os.path.join(project_root, \"models\", \"bi_gru\"),\n",
    "    custom_objects={'PrecisionMulticlass': PrecisionMulticlass,\n",
    "                    'RecallMulticlass': RecallMulticlass,\n",
    "                    'F1ScoreMulticlass': F1ScoreMulticlass,\n",
    "                    'WeightedCategoricalCrossEntropy': partial(WeightedCategoricalCrossEntropy, weights=weights)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "testing_data = tf.data.Dataset.load(os.path.join(project_root, 'datasets', 'training', 'testing_data_2labels_tf'))\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_report(model, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "y_pred_probs = model.predict(testing_data)\n",
    "y_pred = tf.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "y_true = tf.concat([y for x, y in testing_data], axis=0)\n",
    "y_true = tf.argmax(y_true, axis=1)\n",
    "\n",
    "# Find the indices where the predictions are incorrect\n",
    "incorrect_indices = tf.where(y_pred != y_true)\n",
    "incorrect_indices = tf.squeeze(incorrect_indices).numpy()\n",
    "\n",
    "# Find the indices where the predictions are correct\n",
    "corrected_indices = tf.where(y_pred == y_true)\n",
    "corrected_indices = tf.squeeze(corrected_indices).numpy()\n",
    "\n",
    "# Retrieve the corresponding rows from the original dataframe\n",
    "failed_predictions = data.iloc[incorrect_indices].copy()\n",
    "correct_predictions = data.iloc[corrected_indices].copy()\n",
    "\n",
    "# Add predicted probabilities and final predictions as new columns\n",
    "failed_predictions['predicted_probabilities'] = y_pred_probs[incorrect_indices].tolist()\n",
    "failed_predictions['final_prediction'] = y_pred.numpy()[incorrect_indices]\n",
    "correct_predictions['predicted_probabilities'] = y_pred_probs[corrected_indices].tolist()\n",
    "correct_predictions['final_prediction'] = y_pred.numpy()[corrected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot failed predictions distributions\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "# Distribution of tokens_length in failed predictions\n",
    "sns.histplot(failed_predictions['log_tokens_length'], kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"Tokens length in failed predictions\")\n",
    "# Distribution of tokens_length in correct predictions\n",
    "sns.histplot(correct_predictions['log_tokens_length'], kde=True, ax=ax[1])\n",
    "ax[1].set_title(\"Tokens length in correct predictions\")\n",
    "# Distribution of unique_tokens in failed predictions\n",
    "sns.histplot(failed_predictions['unique_tokens'], kde=True, ax=ax[2])\n",
    "ax[2].set_title(\"Unique tokens in failed predictions\")\n",
    "# Distribution of unique_tokens in correct predictions\n",
    "sns.histplot(correct_predictions['unique_tokens'], kde=True, ax=ax[3])\n",
    "ax[3].set_title(\"Unique tokens in correct predictions\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_predictions[failed_predictions.label == failed_predictions.final_prediction].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Set the experiment name (if it doesn't exist, it will be created)\n",
    "mlflow.set_experiment(\"baseline_model_experiment\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"baseline_model\"):\n",
    "\n",
    "\n",
    "    # Save a model\n",
    "    baseline_model_json = baseline_model.to_json()\n",
    "    with open(os.path.join(project_root, \"model_structure.json\"), \"w\") as json_file:\n",
    "        json_file.write(baseline_model_json)\n",
    "        mlflow.log_artifact(os.path.join(project_root, \"model_structure.json\"))  # Save model structure to MLflow\n",
    "        \n",
    "    mlflow.tensorflow.save_model(baseline_model, 'baseline_model',\n",
    "                                 input_example=input_example)\n",
    "\n",
    "    # Save the model's weights\n",
    "    baseline_model.save_weights(os.path.join(project_root, \"model_weights.h5\"))\n",
    "    mlflow.log_artifact(os.path.join(project_root, \"model_weights.h5\"))  # Save weights to MLflow\n",
    "\n",
    "    # Alternatively, you can log the entire model in one go:\n",
    "    mlflow.tensorflow.log_model(baseline_model, \"full_model\")\n",
    "        \n",
    "    # Save embedding matrix\n",
    "    mlflow.log_artifact(os.path.join(project_root, 'models', 'embedding_matrix_300.npy'))\n",
    "        \n",
    "    # Log evaluation metrics\n",
    "    mlflow.log_metric(\"eval_loss\", eval_loss)\n",
    "    mlflow.log_metric(\"eval_precision\", eval_precision)\n",
    "    mlflow.log_metric(\"eval_recall\", eval_recall)\n",
    "\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"vocab_size\", vocab_size)\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", len(baseline_model_history.history['loss']))\n",
    "    mlflow.log_param(\"early_stopping_patience\", early_stopping.patience)\n",
    "    mlflow.log_param(\"early_stopping_min_delta\", early_stopping.min_delta)\n",
    "\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
