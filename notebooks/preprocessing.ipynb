{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>2</td>\n",
       "      <td>4ecc4591238c4855bd54ea0d584f3054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>c682b650f3b24e6b94b36b89acd68e57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>1</td>\n",
       "      <td>9c92c46021824d89b96b0bba2b2b5a83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>c4ab2ea47a3e4e3bbbf530d273cc244f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>1</td>\n",
       "      <td>23e3092360e54bca85a5b0336ed8cf8e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                               text  label  \\\n",
       "0  hate_speech  !!! RT @mayasolovely: As a woman you shouldn't...      2   \n",
       "1  hate_speech  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      1   \n",
       "2  hate_speech  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      1   \n",
       "3  hate_speech  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      1   \n",
       "4  hate_speech  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      1   \n",
       "\n",
       "                                 id  \n",
       "0  4ecc4591238c4855bd54ea0d584f3054  \n",
       "1  c682b650f3b24e6b94b36b89acd68e57  \n",
       "2  9c92c46021824d89b96b0bba2b2b5a83  \n",
       "3  c4ab2ea47a3e4e3bbbf530d273cc244f  \n",
       "4  23e3092360e54bca85a5b0336ed8cf8e  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Adds the parent directory to the path\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(project_root, 'datasets')\n",
    "data_file = os.path.join(data_dir, 'raw/dataset_full.csv')\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utils.cleaning_items import slang_dict, REGEX_REMOVE, REGEX_REPLACE\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "stop_words = stopwords.words('english')\n",
    "combined_corpus = set(words.words()) | set(wordnet.words())\n",
    "combined_corpus = {word.lower() for word in combined_corpus}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:14:57,594: E symspellpy.symspellpy] Dictionary file not found at frequency_dictionary_en_82_765.txt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Load precompiled dictionary file or corpus\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary('frequency_dictionary_en_82_765.txt', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup suggestions for single-word input strings\n",
    "input_term = \"word\"  # misspelling of \"apostrophe\"\n",
    "\n",
    "# max edit distance per lookup\n",
    "# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n",
    "suggestions = sym_spell.lookup(\n",
    "    input_term, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n",
    "    \n",
    "    \n",
    "suggestions[0].term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the input text by applying several preprocessing steps.\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Apply REGEX_REMOVE and REGEX_REPLACE\n",
    "    for pattern in REGEX_REMOVE:\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "    for pattern, repl in REGEX_REPLACE.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "            \n",
    "    # Apply additionnal text cleaning steps\n",
    "    text = re.sub(r'^RT @\\w+: ', '', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'\\b\\w*jpeg\\w*\\b|\\b\\w*jpg\\w*\\b', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'@\\w+', '<PERSON>', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(\\w+)\\b\\s+\\1\\b', '', text)\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'[\\x80-\\xFF]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_text_blob(text: str, stop_words, slang_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Corrects the given text by replacing slang words, removing stop words, and performing spelling correction.\n",
    "    Args:\n",
    "        text (str): The input text to be corrected.\n",
    "        stop_words (set): A set of stop words to be removed from the text.\n",
    "        slang_dict (dict): A dictionary where keys are slang words and values are their corresponding replacements.\n",
    "    Returns:\n",
    "        str: The corrected text after slang replacement, stop word removal, and spelling correction.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    tokens = [slang_dict.get(word, word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    text = ' '.join(tokens)\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text_noblob(text: str, stop_words, slang_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Corrects the given text by replacing slang words, removing stop words, and performing spelling correction.\n",
    "    Args:\n",
    "        text (str): The input text to be corrected.\n",
    "        stop_words (set): A set of stop words to be removed from the text.\n",
    "        slang_dict (dict): A dictionary where keys are slang words and values are their corresponding replacements.\n",
    "    Returns:\n",
    "        str: The corrected text after slang replacement, stop word removal, and spelling correction.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    tokens = [slang_dict.get(word, word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].term for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.sample(100)\n",
    "test_df['text_cleaned'] = test_df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for TextBlob: 38.21 seconds\n",
      "Time taken for symspell: 34.61 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# test with Textblob\n",
    "start_time = time.time()\n",
    "test_df['text_corrected_textblob'] = test_df['text_cleaned'].apply(lambda x: correct_text_blob(x, stop_words, slang_dict))\n",
    "end_time = time.time()\n",
    "time_textblob = end_time - start_time\n",
    "\n",
    "# test with symspell\n",
    "start_time = time.time()\n",
    "test_df['text_corrected_symspell'] = test_df['text_cleaned'].apply(lambda x: correct_text_noblob(x, stop_words, slang_dict))\n",
    "end_time = time.time()\n",
    "time_symspell = end_time - start_time\n",
    "\n",
    "print(f\"Time taken for TextBlob: {time_textblob:.2f} seconds\")\n",
    "print(f\"Time taken for symspell: {time_symspell:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello understand in considers bind nail build three official christian family organizations cos organization within in move cos section need\n",
      "hello i understand that the inc considers the binhi kadiwa and buklod to be the three official christian family organizations but the cws is an organization within the inc we can move the cws to its own section if need be\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text', 'text_cleaned', 'text_corrected_textblob', 'text_corrected_symspell']].iloc[0,3])\n",
    "print(test_df[['text', 'text_cleaned', 'text_corrected_textblob', 'text_corrected_symspell']].iloc[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def lemma_text(tokens: list) -> list:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ls = [lemmatizer.lemmatize(token, 'v') for token in tokens]\n",
    "    ls = [lemmatizer.lemmatize(token, 'n') for token in ls]\n",
    "    ls = [lemmatizer.lemmatize(token, 'a') for token in ls]\n",
    "    return ls\n",
    "\n",
    "test_sentence = \"I am running in the nicest park of the city texts\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(test_sentence, preserve_line=True)\n",
    "lemma_tokens = lemma_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'be', 'run', 'in', 'the', 'nice', 'park', 'of', 'the', 'city', 'text']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
