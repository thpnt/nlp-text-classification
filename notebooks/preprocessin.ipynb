{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>2</td>\n",
       "      <td>4ecc4591238c4855bd54ea0d584f3054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>c682b650f3b24e6b94b36b89acd68e57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>1</td>\n",
       "      <td>9c92c46021824d89b96b0bba2b2b5a83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>c4ab2ea47a3e4e3bbbf530d273cc244f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>1</td>\n",
       "      <td>23e3092360e54bca85a5b0336ed8cf8e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  ...                                id\n",
       "0  hate_speech  ...  4ecc4591238c4855bd54ea0d584f3054\n",
       "1  hate_speech  ...  c682b650f3b24e6b94b36b89acd68e57\n",
       "2  hate_speech  ...  9c92c46021824d89b96b0bba2b2b5a83\n",
       "3  hate_speech  ...  c4ab2ea47a3e4e3bbbf530d273cc244f\n",
       "4  hate_speech  ...  23e3092360e54bca85a5b0336ed8cf8e\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Adds the parent directory to the path\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(project_root, 'datasets')\n",
    "data_file = os.path.join(data_dir, 'raw/merged_dataset.csv')\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data rebalancing - DownSample of Class 0 (not harmful comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    30000\n",
       "1    24897\n",
       "2    14681\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebalance the data by selecting 30000 random samples from class 0\n",
    "df_balanced = pd.concat([df[df.label == 0].sample(n=30000), df[df.label != 0]])\n",
    "df_balanced.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = df_balanced.sample(frac=1).sample(frac=0.1)\n",
    "df = df.sample(frac=0.02)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#import pandas as pd\n",
    "#from textblob import TextBlob\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from setup.utils_setup import slang_dict\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from tqdm import tqdm\n",
    "#\n",
    "#tqdm.pandas()\n",
    "#\n",
    "## Download stopwords if you haven't already\n",
    "#nltk.download('stopwords')\n",
    "#\n",
    "## Define the list of stopwords\n",
    "#stop_words = list(set(stopwords.words('english')))\n",
    "#\n",
    "## Comprehensive text cleaning function\n",
    "#def clean_text(text):\n",
    "#    \"\"\"\n",
    "#    Clean text by performing a series of regex substitutions.\n",
    "#    \"\"\"\n",
    "#    # Step 1: Remove retweet patterns (RT @user:)\n",
    "#    text = re.sub(r'^RT @\\w+: ', '', text)\n",
    "#\n",
    "#    # Step 2: Remove URLs, image formats (jpg, jpeg), and line breaks\n",
    "#    text = re.sub(r'http\\S+', ' ', text)\n",
    "#    text = re.sub(r'\\b\\w*jpeg\\w*\\b|\\b\\w*jpg\\w*\\b', '', text)\n",
    "#    text = re.sub(r'\\n', ' ', text)\n",
    "#\n",
    "#    # Step 3: Replace mentions with a placeholder and remove punctuation\n",
    "#    text = re.sub(r'@\\w+', '<PERSON>', text)\n",
    "#    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#\n",
    "#    # Step 4: Remove numbers, strip whitespaces, and remove repeating words\n",
    "#    text = re.sub(r'\\d+', '', text)\n",
    "#    text = re.sub(r'\\b(\\w+)\\b\\s+\\1\\b', '', text)\n",
    "#    text = text.strip().lower()\n",
    "#    \n",
    "#    # Step 5: Remove special characters and emojis\n",
    "#    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "#    text = re.sub(r'[\\x80-\\xFF]', '', text)\n",
    "#\n",
    "#    return text\n",
    "## Apply text cleaning function in one pass\n",
    "#df['text'] = df['text'].apply(clean_text)\n",
    "#\n",
    "## Function to correct spelling and slang terms\n",
    "#def correct_text(text, stop_words: set, slang_dict: dict = slang_dict):\n",
    "#    \"\"\"\n",
    "#    Replace slang terms and correct spelling in the entire sentence.\n",
    "#    \"\"\"\n",
    "#    # Step 0: Tokenize the text\n",
    "#    tokens = text.split()\n",
    "#    \n",
    "#    # Step 1: Replace slang terms\n",
    "#    tokens = [slang_dict.get(word, word) for word in tokens]\n",
    "#\n",
    "#    # Step 2: Remove stopwords from tokens\n",
    "#    tokens = [word for word in tokens if word not in stop_words]\n",
    "#    \n",
    "#    # Replace text with the corrected tokens\n",
    "#    text = ' '.join(tokens)\n",
    "#    \n",
    "#    # Step 3: Correct spelling using TextBlob on the entire sentence\n",
    "#    corrected_text = str(TextBlob(text).correct())\n",
    "#    \n",
    "#    return corrected_text\n",
    "#\n",
    "## Apply lemmaization\n",
    "#def lemma_text(tokens):\n",
    "#    \"\"\"\n",
    "#    Lemmatize tokens using WordNet.\n",
    "#    \"\"\"\n",
    "#    lemmatizer = WordNetLemmatizer()\n",
    "#    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "#\n",
    "#\n",
    "## Apply text cleaning function in one pass\n",
    "#df['text'] = df['text'].apply(clean_text)\n",
    "## Apply correction\n",
    "#df['text'] = df['text'].progress_apply(lambda x: correct_text(x, stop_words=stop_words))\n",
    "## Tokenize the text\n",
    "#df['tokens'] = df['text'].apply(word_tokenize, preserve_line=True)\n",
    "## Remove rows with empty token lists\n",
    "#df = df[df['tokens'].map(len) > 0]\n",
    "## Apply lemmatization\n",
    "#df['tokens'] = df['tokens'].apply(lemma_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>\"You talk like a fag and your shit's retarded....</td>\n",
       "      <td>1</td>\n",
       "      <td>17e75cb79605490e9c2b732260626754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17648</th>\n",
       "      <td>hate_speech</td>\n",
       "      <td>RT @TheDiLLon1: Y'all not gonna trash Spirit a...</td>\n",
       "      <td>2</td>\n",
       "      <td>ba6732e519e24cf48c15bc4a261727a7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25373</th>\n",
       "      <td>toxic_comment</td>\n",
       "      <td>\"\\n\\n Another crap article \\nParrots what prom...</td>\n",
       "      <td>1</td>\n",
       "      <td>14d43018458d460ea6b19daf64956b87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58898</th>\n",
       "      <td>toxic_comment</td>\n",
       "      <td>You have already been notified about it but di...</td>\n",
       "      <td>0</td>\n",
       "      <td>f3d1e9a8c4894bf180bc01e9a174c3e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53559</th>\n",
       "      <td>toxic_comment</td>\n",
       "      <td>\":\"\"Racist\"\" is a slur, and implying Jewish ow...</td>\n",
       "      <td>1</td>\n",
       "      <td>efe0901b93de438ab97e15b7ed5f3ea1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source                                               text  \\\n",
       "615      hate_speech  \"You talk like a fag and your shit's retarded....   \n",
       "17648    hate_speech  RT @TheDiLLon1: Y'all not gonna trash Spirit a...   \n",
       "25373  toxic_comment  \"\\n\\n Another crap article \\nParrots what prom...   \n",
       "58898  toxic_comment  You have already been notified about it but di...   \n",
       "53559  toxic_comment  \":\"\"Racist\"\" is a slur, and implying Jewish ow...   \n",
       "\n",
       "       label                                id  \n",
       "615        1  17e75cb79605490e9c2b732260626754  \n",
       "17648      2  ba6732e519e24cf48c15bc4a261727a7  \n",
       "25373      1  14d43018458d460ea6b19daf64956b87  \n",
       "58898      0  f3d1e9a8c4894bf180bc01e9a174c3e1  \n",
       "53559      1  efe0901b93de438ab97e15b7ed5f3ea1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source', 'text', 'label', 'id'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.70it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.19it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.85it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.62it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.preprocessing import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(batch_size=10)\n",
    "\n",
    "# Clean the text with tokenizer\n",
    "df = tokenizer.clean(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in combined corpus: 331327\n",
      "Total unique words in WordNet: 147306\n",
      "Total unique words in Word: 323592\n",
      "Original sentence: caring about details dogs running bottles bottling hehzhz aha\n",
      "Original tokens: ['caring', 'about', 'details', 'dogs', 'running', 'bottles', 'bottling', 'hehzhz', 'aha']\n",
      "final tokens: ['care', 'about', 'detail', 'dog', 'run', 'bottle', 'bottle', '<UNK>', 'aha']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords, wordnet, brown, gutenberg\n",
    "\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "# Create sets of words from each corpus for faster lookup\n",
    "words_corpus = set(words.words())\n",
    "wordnet_corpus = set(wordnet.words())\n",
    "#brown_corpus = set(brown.words())\n",
    "#gutenberg_corpus = set(gutenberg.words())\n",
    "\n",
    "# Combine the three corpora\n",
    "combined_corpus = words_corpus | wordnet_corpus #| brown_corpus | gutenberg_corpus\n",
    "\n",
    "# Convert to lowercase for case-insensitive matching\n",
    "combined_corpus = {word.lower() for word in combined_corpus}\n",
    "\n",
    "print(f\"Total unique words in combined corpus: {len(combined_corpus)}\")\n",
    "print(f\"Total unique words in WordNet: {len(wordnet_corpus)}\")\n",
    "print(f\"Total unique words in Word: {len(words_corpus)}\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"caring about details dogs running bottles bottling hehzhz aha\"\n",
    "# Example function to mark unknown words\n",
    "def replace_unknown_tokens(tokens, unknown_token=\"<UNK>\"):\n",
    "    tokens = [token if token.lower() in combined_corpus else unknown_token for token in tokens]\n",
    "    return tokens\n",
    "tokens = word_tokenize(sentence, preserve_line=True)\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token, 'v') for token in tokens]\n",
    "final_tokens = [token if token.lower() in combined_corpus else \"<UNK>\" for token in lemmatized_tokens]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"final tokens: {final_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word frequency analysis (in sentences and in corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
